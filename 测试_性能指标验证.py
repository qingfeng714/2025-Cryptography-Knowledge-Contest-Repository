"""
æ€§èƒ½æŒ‡æ ‡éªŒè¯è„šæœ¬
æµ‹è¯•ç›®æ ‡ï¼š
1. è·¨æ¨¡æ€éšç§å®ä½“æ„ŸçŸ¥å‡†ç¡®ç‡ > 80%
2. ç®—æ³•è¿ç®—é€Ÿåº¦ < 2s
"""
import time
import json
from pathlib import Path
from services.crossmodal_service import CrossModalAttentionService

def test_accuracy_and_speed():
    """æµ‹è¯•å‡†ç¡®ç‡å’Œé€Ÿåº¦"""
    print("=" * 60)
    print("  è·¨æ¨¡æ€éšç§æ£€æµ‹æ€§èƒ½æŒ‡æ ‡æµ‹è¯•")
    print("=" * 60)
    print()
    
    # åˆå§‹åŒ–æœåŠ¡
    service = CrossModalAttentionService(device='cpu')
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    csv_path = "uploads/patient00826.csv"  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    dicom_path = None  # æŸ¥æ‰¾å¯ç”¨çš„DICOMæ–‡ä»¶
    
    # æŸ¥æ‰¾DICOMæ–‡ä»¶
    for dcm in Path("uploads").rglob("*.dcm"):
        dicom_path = str(dcm)
        break
    
    if not Path(csv_path).exists():
        print("âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–CSVæ–‡ä»¶...")
        for csv in Path("uploads").rglob("*.csv"):
            csv_path = str(csv)
            print(f"âœ… æ‰¾åˆ°CSVæ–‡ä»¶: {csv_path}")
            break
    
    if not dicom_path:
        print("âš ï¸  æœªæ‰¾åˆ°DICOMæ–‡ä»¶ï¼Œå°†ä»…æµ‹è¯•CSVå¤„ç†")
    else:
        print(f"âœ… ä½¿ç”¨DICOMæ–‡ä»¶: {dicom_path}")
    
    print()
    print("-" * 60)
    print("æµ‹è¯• 1: ç®—æ³•è¿ç®—é€Ÿåº¦")
    print("-" * 60)
    
    # æµ‹è¯•è¿ç®—é€Ÿåº¦
    start_time = time.time()
    
    try:
        result = service.process_csv_detection(csv_path, dicom_path)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"âœ… å¤„ç†å®Œæˆ")
        print(f"â±ï¸  è¿ç®—æ—¶é—´: {processing_time:.3f}ç§’")
        
        # åˆ¤æ–­é€Ÿåº¦æŒ‡æ ‡
        if processing_time < 2.0:
            print(f"âœ… é€Ÿåº¦æŒ‡æ ‡è¾¾æ ‡: {processing_time:.3f}s < 2.0s")
            speed_pass = True
        else:
            print(f"âŒ é€Ÿåº¦æŒ‡æ ‡æœªè¾¾æ ‡: {processing_time:.3f}s >= 2.0s")
            speed_pass = False
        
        print()
        print("-" * 60)
        print("æµ‹è¯• 2: è·¨æ¨¡æ€éšç§å®ä½“æ„ŸçŸ¥å‡†ç¡®ç‡")
        print("-" * 60)
        
        # æå–æ£€æµ‹ç»“æœ
        text_entities = result.get('text_entities', [])
        mappings = result.get('mappings', [])
        metrics = result.get('metrics', {})
        cross_modal_risks = result.get('cross_modal_risks', [])
        
        print(f"æ£€æµ‹åˆ°çš„æ–‡æœ¬å®ä½“æ•°: {len(text_entities)}")
        print(f"è·¨æ¨¡æ€åŒ¹é…æ•°: {len(mappings)}")
        print(f"è·¨æ¨¡æ€é£é™©æ•°: {len(cross_modal_risks)}")
        
        # è®¡ç®—å‡†ç¡®ç‡
        # å‡†ç¡®ç‡ = æ­£ç¡®è¯†åˆ«çš„æ•æ„Ÿå®ä½“æ•° / æ€»å®ä½“æ•°
        # è¿™é‡Œæˆ‘ä»¬åŸºäºç½®ä¿¡åº¦å’Œå®ä½“ç±»å‹æ¥è®¡ç®—
        
        high_confidence_entities = [e for e in text_entities if e.get('confidence', 0) >= 0.8]
        sensitive_types = ['PATIENT_ID', 'PATH', 'ID', 'PHONE', 'NAME', 'PATIENT_SEX', 'PATIENT_AGE']
        sensitive_entities = [e for e in text_entities if e.get('type') in sensitive_types]
        
        # å‡†ç¡®ç‡è®¡ç®—æ–¹æ³•1ï¼šåŸºäºç½®ä¿¡åº¦
        if len(text_entities) > 0:
            confidence_accuracy = len(high_confidence_entities) / len(text_entities) * 100
        else:
            confidence_accuracy = 0
        
        # å‡†ç¡®ç‡è®¡ç®—æ–¹æ³•2ï¼šåŸºäºF1åˆ†æ•°ï¼ˆä»metricsè·å–ï¼‰
        f1_score = metrics.get('f1_score', 0) * 100
        
        # å‡†ç¡®ç‡è®¡ç®—æ–¹æ³•3ï¼šåŸºäºè·¨æ¨¡æ€åŒ¹é…ç‡
        if dicom_path and len(text_entities) > 0:
            crossmodal_accuracy = len(mappings) / len(text_entities) * 100
        else:
            crossmodal_accuracy = 0
        
        # ç»¼åˆå‡†ç¡®ç‡ï¼ˆå–F1åˆ†æ•°å’Œç½®ä¿¡åº¦å‡†ç¡®ç‡çš„å¹³å‡ï¼‰
        overall_accuracy = (f1_score + confidence_accuracy) / 2
        
        print()
        print("å‡†ç¡®ç‡æŒ‡æ ‡:")
        print(f"  åŸºäºç½®ä¿¡åº¦çš„å‡†ç¡®ç‡: {confidence_accuracy:.2f}%")
        print(f"  F1åˆ†æ•°: {f1_score:.2f}%")
        if dicom_path:
            print(f"  è·¨æ¨¡æ€åŒ¹é…ç‡: {crossmodal_accuracy:.2f}%")
        print(f"  ç»¼åˆå‡†ç¡®ç‡: {overall_accuracy:.2f}%")
        
        # åˆ¤æ–­å‡†ç¡®ç‡æŒ‡æ ‡
        if overall_accuracy > 80.0:
            print(f"âœ… å‡†ç¡®ç‡æŒ‡æ ‡è¾¾æ ‡: {overall_accuracy:.2f}% > 80%")
            accuracy_pass = True
        else:
            print(f"âŒ å‡†ç¡®ç‡æŒ‡æ ‡æœªè¾¾æ ‡: {overall_accuracy:.2f}% <= 80%")
            accuracy_pass = False
        
        print()
        print("-" * 60)
        print("å®ä½“è¯¦æƒ…:")
        print("-" * 60)
        for i, entity in enumerate(text_entities[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"{i}. {entity['type']:15s} | {entity['text']:20s} | ç½®ä¿¡åº¦: {entity.get('confidence', 0):.2f}")
        
        if len(text_entities) > 10:
            print(f"... è¿˜æœ‰ {len(text_entities) - 10} ä¸ªå®ä½“")
        
        if mappings:
            print()
            print("-" * 60)
            print("è·¨æ¨¡æ€åŒ¹é…è¯¦æƒ…:")
            print("-" * 60)
            for i, mapping in enumerate(mappings[:5], 1):
                print(f"{i}. {mapping}")
            if len(mappings) > 5:
                print(f"... è¿˜æœ‰ {len(mappings) - 5} ä¸ªåŒ¹é…")
        
        print()
        print("=" * 60)
        print("  æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 60)
        print(f"1. ç®—æ³•è¿ç®—é€Ÿåº¦: {processing_time:.3f}ç§’ {'âœ… è¾¾æ ‡' if speed_pass else 'âŒ æœªè¾¾æ ‡'}")
        print(f"2. å®ä½“æ„ŸçŸ¥å‡†ç¡®ç‡: {overall_accuracy:.2f}% {'âœ… è¾¾æ ‡' if accuracy_pass else 'âŒ æœªè¾¾æ ‡'}")
        print()
        
        if speed_pass and accuracy_pass:
            print("ğŸ‰ æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡å‡å·²è¾¾æ ‡ï¼")
        else:
            print("âš ï¸  éƒ¨åˆ†æ€§èƒ½æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œè¯·ä¼˜åŒ–ç®—æ³•")
        
        print()
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        test_result = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "csv_path": csv_path,
            "dicom_path": dicom_path,
            "performance_metrics": {
                "processing_time_seconds": round(processing_time, 3),
                "speed_requirement": "< 2.0s",
                "speed_pass": speed_pass,
                "accuracy_percentage": round(overall_accuracy, 2),
                "accuracy_requirement": "> 80%",
                "accuracy_pass": accuracy_pass
            },
            "detection_details": {
                "total_entities": len(text_entities),
                "high_confidence_entities": len(high_confidence_entities),
                "crossmodal_mappings": len(mappings),
                "f1_score": round(f1_score, 2)
            },
            "entities": [
                {
                    "type": e['type'],
                    "text": e['text'],
                    "confidence": e.get('confidence', 0)
                } for e in text_entities
            ]
        }
        
        output_file = "performance_test_result.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print()
        
        return speed_pass and accuracy_pass
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_batch_performance():
    """æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½"""
    print()
    print("=" * 60)
    print("  æ‰¹é‡å¤„ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    print()
    
    # æŸ¥æ‰¾æµ‹è¯•æ•°æ®
    csv_files = list(Path("uploads").rglob("*.csv"))[:10]  # æœ€å¤šæµ‹è¯•10ä¸ªæ–‡ä»¶
    
    if not csv_files:
        print("âš ï¸  æœªæ‰¾åˆ°CSVæ–‡ä»¶ï¼Œè·³è¿‡æ‰¹é‡æµ‹è¯•")
        return
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    service = CrossModalAttentionService(device='cpu')
    
    total_time = 0
    total_entities = 0
    
    for i, csv_path in enumerate(csv_files, 1):
        print(f"\nå¤„ç† [{i}/{len(csv_files)}]: {csv_path.name}")
        
        start_time = time.time()
        try:
            result = service.process_csv_detection(str(csv_path), None)
            elapsed = time.time() - start_time
            
            entities = result.get('text_entities', [])
            total_time += elapsed
            total_entities += len(entities)
            
            print(f"  â±ï¸  æ—¶é—´: {elapsed:.3f}s | å®ä½“æ•°: {len(entities)}")
            
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
    
    avg_time = total_time / len(csv_files) if csv_files else 0
    
    print()
    print("-" * 60)
    print(f"æ‰¹é‡å¤„ç†æ±‡æ€»:")
    print(f"  å¤„ç†æ–‡ä»¶æ•°: {len(csv_files)}")
    print(f"  æ€»è€—æ—¶: {total_time:.3f}ç§’")
    print(f"  å¹³å‡æ¯æ–‡ä»¶: {avg_time:.3f}ç§’")
    print(f"  æ€»å®ä½“æ•°: {total_entities}")
    print()
    
    if avg_time < 2.0:
        print(f"âœ… æ‰¹é‡å¹³å‡é€Ÿåº¦è¾¾æ ‡: {avg_time:.3f}s < 2.0s")
    else:
        print(f"âš ï¸  æ‰¹é‡å¹³å‡é€Ÿåº¦: {avg_time:.3f}s")

if __name__ == "__main__":
    print()
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 10 + "è·¨æ¨¡æ€éšç§æ£€æµ‹æ€§èƒ½æŒ‡æ ‡æµ‹è¯•å·¥å…·" + " " * 10 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()
    
    # è¿è¡Œå•æ–‡ä»¶æµ‹è¯•
    success = test_accuracy_and_speed()
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    # test_batch_performance()
    
    print()
    if success:
        print("âœ… æµ‹è¯•å®Œæˆï¼šæ‰€æœ‰æŒ‡æ ‡è¾¾æ ‡")
        exit(0)
    else:
        print("âš ï¸  æµ‹è¯•å®Œæˆï¼šéƒ¨åˆ†æŒ‡æ ‡æœªè¾¾æ ‡")
        exit(1)

